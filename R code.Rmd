---
title: "Untitled"
author: "Sofia"
date: "11/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Importing file and libraries

First we will read the file, import the tidyverse library to manipulate the dataset and reticulate to be able to play with python as well. First we are going to drop the columns X and X.1

```{r}
data <- read.csv("ENB2012_data (2).csv")

library(tidyverse) 
library(reticulate) 

use_python("/usr/bin/python3", required = F)

# do not touch these settings
Sys.which("python")
Sys.getenv("DISPLAY") 

matplotlib <- import("matplotlib", convert = TRUE)
matplotlib$use("Agg")

data %>% select(-X, -X.1) %>% na.omit() -> data

install.packages("gridExtra")
library("gridExtra")

```

```{r}


plot1 <- ggplot(data, (aes(x = X1, y = Y1))) + geom_point() 
plot2 <- ggplot(data, (aes(x = X2, y = Y1))) + geom_point() 
plot3 <- ggplot(data, (aes(x = X3, y = Y1))) + geom_point() 
plot4 <- ggplot(data, (aes(x = X4, y = Y1))) + geom_point() 
plot5 <- ggplot(data, (aes(x = X5, y = Y1))) + geom_point() 
plot6 <- ggplot(data, (aes(x = X6, y = Y1))) + geom_point() 
plot7 <- ggplot(data, (aes(x = X7, y = Y1))) + geom_point() 
plot8 <- ggplot(data, (aes(x = X8, y = Y1))) + geom_point() 

grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8, ncol=4)

plot11 <- ggplot(data, (aes(x = X1, y = Y2))) + geom_point() 
plot22 <- ggplot(data, (aes(x = X2, y = Y2))) + geom_point() 
plot33 <- ggplot(data, (aes(x = X3, y = Y2))) + geom_point() 
plot44 <- ggplot(data, (aes(x = X4, y = Y2))) + geom_point() 
plot55 <- ggplot(data, (aes(x = X5, y = Y2))) + geom_point() 
plot66 <- ggplot(data, (aes(x = X6, y = Y2))) + geom_point() 
plot77 <- ggplot(data, (aes(x = X7, y = Y2))) + geom_point() 
plot88 <- ggplot(data, (aes(x = X8, y = Y2))) + geom_point() 

grid.arrange(plot11, plot22, plot33, plot44, plot55, plot66, plot77, plot88, ncol=4)




```
```{r}
library(tidyverse)
library(cluster)

cormat <- data %>% cor() %>% as.data.frame %>% rownames_to_column("var1")
tidycor <- cormat %>% pivot_longer(-1, names_to = "var2", values_to = "correlation")
tidycor

tidycor %>% ggplot(aes(var1, var2, fill=correlation)) + 
  geom_tile() + 
  scale_fill_gradient2(low="red", mid="white", high = "blue") +
  geom_text(aes(label=round(correlation,2)),color = "black", size = 4)+ #overlays correlation values
  theme(axis.text.x = element_text(angle = 90)) + #flips the x-axis labels
  coord_fixed()
```

```{r}
data %>% select (X1, X2, X3, X4, X5, X6, X7, X8) -> X
data %>% select (Y1) -> Y1
data %>% select (Y2) -> Y2
```


## Modelling

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn.model_selection import train_test_split

#First we will do it to predict the y1 response variable
X_train, X_test, y1_train, y1_test = train_test_split(r.X, r.Y1, random_state=0)
reg1 = linear_model.LinearRegression()
reg1.fit(X_train, y1_train)
y_pred1_reg = reg1.predict(X_test)
mse_1_reg = np.mean((y1_test - y_pred1_reg)**2)

figure = plt.figure(figsize=(15, 11))

#subplot for r1 regression
ax = plt.subplot(1,2,1)
plt.title('Linear regression Y1')
plt.xlabel('Actual values Y1', size=12)
plt.ylabel('Predicted Y1', size=12)
plt.plot(y1_test,y_pred1_reg , '.b')


#Now we will do it to predict the y2 response variable

X_train, X_test, y2_train, y2_test = train_test_split(r.X, r.Y2, random_state=0)

reg2 = linear_model.LinearRegression()
reg2.fit(X_train, y2_train)
y_pred2_reg = reg2.predict(X_test)
mse_2_reg = np.mean((y2_test - y_pred2_reg)**2)


#subplot for r1 regression
ax = plt.subplot(1,2,2)
plt.title('Linear regression Y2')
plt.xlabel('Actual values Y2', size=12)
plt.ylabel('Predicted Y2', size=12)
plt.plot(y2_test,y_pred2_reg , '.b', color = 'red')


#Xlin = np.linspace(0, 25, 100)
#Xlin1 = np.linspace(0, 25, 100)[:, np.newaxis]
#Xlin2 = np.linspace(0, 25, 100)[:, np.newaxis]
#Xlin3 = np.linspace(0, 25, 100)[:, np.newaxis]
#Xlin4 = np.linspace(0, 25, 100)[:, np.newaxis]
#Xlin5 = np.linspace(0, 25, 100)[:, np.newaxis]
#Xlin6 = np.linspace(0, 25, 100)[:, np.newaxis]
#Xlin7 = np.linspace(0, 25, 100)[:, np.newaxis]

#ylin = reg1.predict(Xlin)


```

Do it with SVM, knn, decision tree and random forests and say which one is better



```{python}
from sklearn.svm import SVR
from sklearn.metrics import accuracy_score

SVM1 = SVR()
SVM1.fit(X_train, y1_train)
y_pred1_svm = reg1.predict(X_test)
mse_1_svr = np.mean((y1_test - y_pred1_svm)**2)

SVM2 = SVR()
SVM1.fit(X_train, y2_train)
y_pred2_svm = reg1.predict(X_test)
mse_2_svr = np.mean((y2_test - y_pred2_svm)**2)

print(mse_1_svr)
print(mse_2_svr)

#ask which metric to use to replace accuracy

```






